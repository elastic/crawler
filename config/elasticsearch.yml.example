## ================== Crawler Configuration - Elasticsearch ====================
#
##  Elasticsearch connection settings. These can be defined individually
##    for a crawl in the crawl config file.
#
##  NOTE: Most Crawler configurations comes with reasonable defaults.
##       Before adjusting the configuration, make sure you understand what you
##       are trying to accomplish and the consequences.
#
## ------------------------------- Elasticsearch -------------------------------
#
## The host of the Elasticsearch deployment, excluding the port.
##    If your Elasticsearch is running on Docker and is on the same machine as
##    Crawler, this value should be `http://host.docker.internal`
#elasticsearch.host: http://localhost
#
#
## The port of the Elasticsearch deployment host.
#elasticsearch.port: 9200
#
#
## The encoded API key for Elasticsearch connection.
##    Using `api_key` is recommended instead of `username`/`password`. Ensure
##    this API key has read and write access to the configured `output_index`
##    in the Crawler config. The api key can be read from the environment
##    using <%= ENV['ES_API_KEY'] %>
#elasticsearch.api_key: 1234
#
#
##  The username for the Elasticsearch connection.
##    Using `username` requires `password` to also be configured.
##    However, `elasticsearch.api_key` is the recommended configuration choice.
#elasticsearch.username: elastic
#
#
##  The password for the Elasticsearch connection.
##    Using `password` requires `username` to also be configured. However, 
##    `elasticsearch.api_key` is the recommended configuration choice. The 
##    password can be read from the environment using <%= ENV['ES_PASSWORD'] %>
#elasticsearch.password: changeme
#
#
##  The SHA256 CA cert fingerprint used to verify SSL connection to Elasticsearch.
##    SSL usage is configured by the presence of `https` in `elasticsearch.host`
#elasticsearch.ca_fingerprint: null
#
#
## ------------------------------- Client Behavior --------------------------------
#
#
## Request timeout in seconds for individual Elasticsearch requests.
##    Default: 10
#elasticsearch.request_timeout: 10
#
#
## Number of retries on failure (e.g., 3), or false/0 to disable retries.
##    If set to `true`, it defaults to 3 retries.
##    Default: 3
#elasticsearch.retry_on_failure: 3
#
#
## The amount of time to wait between requests after a failure occurs.
##    Default: 2000
#elasticsearch.delay_on_retry: 10000
#
#
## Whether to reload connections (refresh node list) after a connection failure.
##    Default: false
#elasticsearch.reload_on_failure: false
#
#
## Enable gzip compression for requests sent to Elasticsearch.
##    Defaults to `true` if not specified. Set to `false` to disable.
#elasticsearch.compression: true
#
#
## ------------------------------- Bulk API -----------------------------------
#
##  The max size of the bulk queue
#elasticsearch.bulk_api.max_items: 10
#
#
##  The max size in bytes of the bulk queue.
##    When it's reached, the Crawler performs a bulk index request to
##    Elasticsearch, and the queue is flushed
#elasticsearch.bulk_api.max_size_bytes: 1_048_576
#
#
## ------------------------------- Pipelines ----------------------------------
#
#
##  The name of the ingest pipeline
##    If pipelines are enabled and this value is `null`,
##    the pipeline `ent-search-generic-ingestion` will be used
#elasticsearch.pipeline: ent-search-generic-ingestion
#
#
##  Whether or not pipelines are enabled
#elasticsearch.pipeline_enabled: true
#
#
##  Enable for the pipeline to reduce whitespace on indexed docs
#elasticsearch.pipeline_params._reduce_whitespace: true
#
#
##  Enable for the pipeline to run ML inference on indexed docs
#elasticsearch.pipeline_params._run_ml_inference: true
#
#
##  Enable for the pipeline to extract binary content from
##    the `_attachment` field of an indexed doc
#elasticsearch.pipeline_params._extract_binary_content: true
