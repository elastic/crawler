# Logging

## Configure your Crawler YAML to write system and/or event logs to file
By default, Crawler will output system logs to `stdout`. You have the option to write these logs to file, in addition to stdout, by setting the following in your Crawler YAML config:

```
    system_logs_to_file: true
```

Crawler also is capable of generating 'event' logs that contain details regarding each crawl being performed. These are meant for debugging and are quite verbose, and as such are only available to be written to file. You can enable these via the following:

```
    event_logs_to_file: true
```
By default, the above log files will be written to a 'logs/' folder located at the top level of your Crawler directory, however this is can be changed in your configuration file via the following:

```
    log_file_directory: "path/to/logs"
```
**If you are using Crawler in a Docker container via the provided docker-compose.yaml**, do not set `log_file_directory`. Simply edit the `docker-compose.yaml` file by uncommenting the following line to enable log files to be written to an external mounted directory called `logs/`:

```
    volumes:
      ...
      - ./logs:/home/app/logs # Enable this to access log files outside the Docker container
```
_If you do not use the provided `docker-compose.yaml`, it is still highly recommended to mount a volume to allow logs to be accessed externally from the running container._

Finally, you can set up a log rotation policy that affects both the system and event logs via the following:

```
    log_file_rotation_policy: 'weekly'
```

The options are 'daily', 'weekly', or 'monthly', with a default value of 'weekly'.

## Setting up Filebeat with the included Filebeat YAML
Crawler provides a basic [Filebeat configuration](config/filebeat.yml.example) to help you kickstart ingesting Crawler's logs into Elasticsearch.
Before using it, don't forget to edit the following configuration values inside the Filebeat YAML file.

_Note, the file names `crawler_event.log` and `crawler_system.log` are generated by Crawler, so make sure they are present in your path!_
```
# these are under filebeat.inputs
paths:
    - "/path/to/opencrawler/crawler_event.log"

...

paths:
    - "/path/to/opencrawler/crawler_system.log"

output.elasticsearch:
  hosts: [""]
  api_key: "id:api_key"
```
If you are running Crawler inside of a Docker container and you have set it up to write logs to a mounted volume, you can simply point the `paths` fields to the directory your logs are being written to.

Check out [Filebeat's quickstart guide](https://www.elastic.co/guide/en/beats/filebeat/current/index.html) to learn more about how to get it installed and running.
