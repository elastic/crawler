./crawler/event_generator.rb:    delegate :system_logger, to: :config
./crawler/event_generator.rb:      system_logger.error("Crawl Error: #{full_message} #{backtrace}")
./crawler/event_generator.rb:      system_logger.debug("#{action} with the following configuration: #{config}")
./crawler/event_generator.rb:      system_logger.info("Finished a crawl stage. Result: #{outcome}; #{message}")
./crawler/event_generator.rb:      system_logger.info("Finished a crawl. Result: #{outcome}; #{message}")
./crawler/event_generator.rb:      system_logger.info(crawl_status_for_system_log(status))
./crawler/event_generator.rb:      system_logger.debug(
./crawler/event_generator.rb:      system_logger.debug("Fetched a page '#{url}' with a status code #{status_code} and an outcome of '#{outcome}'")
./crawler/event_generator.rb:      system_logger_severity = outcome.to_s == 'success' ? Logger::INFO : Logger::WARN
./crawler/event_generator.rb:      system_logger.add(
./crawler/event_generator.rb:        system_logger_severity,
./crawler/http_executor.rb:      @logger = config.system_logger.tagged(:http)
./crawler/cli/schedule.rb:        crawl_config.system_logger.info("Crawler initialized with a cron schedule of #{pattern}")
./crawler/cli/schedule.rb:          crawl_config.system_logger.info(
./crawler/cli/schedule.rb:          crawl_config.system_logger.info(
./crawler/api/config.rb:      attr_reader :system_logger # for free-text logging
./crawler/api/config.rb:        system_logger.debug("Loading SSL certificate: #{file_name.inspect}")
./crawler/api/config.rb:        alt_system_logger = Crawler::CrawlLogger.new(LOG_LEVELS[log_level])
./crawler/api/config.rb:        alt_system_logger.info("THIS IS A TEST MESSAGE")
./crawler/api/config.rb:        system_logger = Logger.new('crawler_system.log', 'weekly')
./crawler/api/config.rb:        system_logger.level = LOG_LEVELS[log_level]
./crawler/api/config.rb:        system_logger.formatter = proc do |_severity, datetime, _progname, msg|
./crawler/api/config.rb:        tagged_system_logger = StaticallyTaggedLogger.new(system_logger)
./crawler/api/config.rb:        @system_logger = tagged_system_logger.tagged("crawl:#{crawl_id}", crawl_stage)
./crawler/api/crawl.rb:      delegate :system_logger, :events, :stats, to: :config
./crawler/api/crawl.rb:        system_logger.info(
./crawler/api/crawl.rb:          system_logger.info('Not removing the crawl queue to allow the crawl to resume later')
./crawler/api/crawl.rb:          system_logger.info('Releasing resources used by the crawl...')
./crawler/rule_engine/base.rb:      delegate :crawl_rules, :domain_allowlist, :robots_txt_service, :system_logger, to: :config
./crawler/rule_engine/base.rb:          system_logger.warn("Timeout while applying a crawl rule  to URL #{url}")
./crawler/rule_engine/base.rb:          system_logger.debug(
./crawler/output_sink/base.rb:      delegate :crawl_id, :document_mapper, :events, :system_logger, to: :config
./crawler/output_sink/elasticsearch.rb:        system_logger.info(
./crawler/output_sink/elasticsearch.rb:        system_logger.info(
./crawler/output_sink/elasticsearch.rb:        system_logger.info("Failed to reach ES at #{es_host}")
./crawler/output_sink/elasticsearch.rb:          system_logger.info("Index [#{config.output_index}] did not exist, but was successfully created!")
./crawler/output_sink/elasticsearch.rb:          system_logger.info("Index [#{config.output_index}] was found!")
./crawler/output_sink/elasticsearch.rb:        raise Errors::ExitIfUnableToCreateIndex, system_logger.info("Failed to create #{config.output_index}") unless
./crawler/output_sink/elasticsearch.rb:          system_logger.debug("Added doc #{doc[:id]} to bulk queue. Current stats: #{operation_queue.current_stats}")
./crawler/output_sink/elasticsearch.rb:        system_logger.debug(
./crawler/output_sink/elasticsearch.rb:        system_logger.info('Deleting docs for pages that were not accessible during the purge crawl.')
./crawler/output_sink/elasticsearch.rb:        system_logger.debug("Full delete query: #{query}")
./crawler/output_sink/elasticsearch.rb:        system_logger.debug("Delete by query response: #{response}")
./crawler/output_sink/elasticsearch.rb:        system_logger.info(msg)
./crawler/output_sink/elasticsearch.rb:          system_logger.debug('Queue was empty when attempting to flush.')
./crawler/output_sink/elasticsearch.rb:        system_logger.info("Sending bulk request with #{indexing_docs_count} items and resetting queue...")
./crawler/output_sink/elasticsearch.rb:          system_logger.info("Successfully indexed #{indexing_docs_count} docs.")
./crawler/output_sink/elasticsearch.rb:          system_logger.warn("Bulk index failed: #{e}")
./crawler/output_sink/elasticsearch.rb:          system_logger.warn("Bulk index failed for unexpected reason: #{e}")
./crawler/output_sink/elasticsearch.rb:        @client ||= ES::Client.new(es_config, system_logger, Crawler.version, crawl_id)
./crawler/data/url_queue/base.rb:        delegate :crawl_id, :system_logger, to: :config
./crawler/data/url_queue/base.rb:          system_logger.warn(message.squish.squeeze(' '))
./crawler/data/url_queue/memory_only.rb:          system_logger.info("Initialized an in-memory URL queue for up to #{memory_size_limit} URLs")
./crawler/logging/stdout.rb:        system_logger = Logger.new($stdout)
./crawler/logging/stdout.rb:        system_logger.level = log_level
./crawler/logging/stdout.rb:        system_logger.formatter = proc do |_severity, datetime, _progname, msg|
./crawler/logging/stdout.rb:        @system_logger = system_logger
./crawler/logging/stdout.rb:        # tagged_system_logger = StaticallyTaggedLogger.new(system_logger)
./crawler/logging/stdout.rb:        # @system_logger = tagged_system_logger.tagged("crawl:#{crawl_id}", crawl_stage)
./crawler/logging/stdout.rb:          @system_logger.debug(message)
./crawler/logging/stdout.rb:          @system_logger.info(message)
./crawler/logging/stdout.rb:          @system_logger.warn(message)
./crawler/logging/stdout.rb:          @system_logger.error(message)
./crawler/logging/stdout.rb:          @system_logger.fatal(message)
./crawler/coordinator.rb:    delegate :events, :system_logger, :config, :executor, :sink, :rule_engine,
./crawler/coordinator.rb:      system_logger.info('Closing the output sink before finishing the crawl...')
./crawler/coordinator.rb:      system_logger.info('Crawl shutdown complete')
./crawler/coordinator.rb:      system_logger.info("Starting the primary crawl with up to #{task_executors.max_length} parallel thread(s)...")
./crawler/coordinator.rb:        system_logger.info('No documents were found for the purge crawl. Skipping purge crawl.')
./crawler/coordinator.rb:      system_logger.info("Starting the purge crawl with up to #{task_executors.max_length} parallel thread(s)...")
./crawler/coordinator.rb:        system_logger.warn("Purge crawls are not supported for sink type #{config.output_sink}. Skipping purge crawl.")
./crawler/coordinator.rb:        system_logger.warn('Purge crawls are disabled in the config file. Skipping purge crawl.')
./crawler/coordinator.rb:      system_logger.debug("Crawl task progress: #{progress_message}")
./crawler/coordinator.rb:        system_logger.debug("Registering robots.txt result for #{domain}: #{crawl_result}")
./crawler/coordinator.rb:        system_logger.warn(
./crawler/coordinator.rb:        system_logger.warn("Error while fetching robots.txt for #{domain}: #{crawl_result.error}")
./crawler/coordinator.rb:        system_logger.debug("Fetched robots.txt for #{domain} from '#{crawl_result.url}'")
./crawler/coordinator.rb:      system_logger.debug("Seeding the crawl with #{config.seed_urls.size} URLs...")
./crawler/coordinator.rb:        system_logger.debug("Seeding the crawl with #{config.sitemap_urls.count} Sitemap URLs...")
./crawler/coordinator.rb:      system_logger.debug(
./crawler/coordinator.rb:      system_logger.debug("Seeding the crawl with #{urls.size} URLs from the ES index...")
./crawler/coordinator.rb:          system_logger.warn("Skipping auto-discovered Sitemap URL #{sitemap} with unsupported URL scheme")
./crawler/coordinator.rb:        system_logger.info("Crawl queue is empty, finishing the #{@crawl_stage} crawl")
./crawler/coordinator.rb:        system_logger.warn("Shutting down the #{@crawl_stage} crawl with #{crawl_queue.length} unprocessed URLs...")
./crawler/coordinator.rb:        system_logger.warn(outcome_message)
./crawler/coordinator.rb:          system_logger.debug('No executors available, sleeping for a second...')
./crawler/coordinator.rb:        system_logger.warn("Failed to schedule a crawl task: #{e}. Going to retry in a second...")
./crawler/coordinator.rb:      system_logger.error("Unexpected error while executing a crawl task #{crawl_task.inspect}: #{e.full_message}")
./crawler/coordinator.rb:          system_logger.warn(
./crawler/coordinator.rb:      system_logger.warn("Too many links in a sitemap '#{crawl_result.url}': #{error}") if limit_reached
./crawler/coordinator.rb:            system_logger.warn(
./crawler/coordinator.rb:      system_logger.warn("Too many links on the page '#{crawl_result.url}'") if limit_reached
./crawler/coordinator.rb:            system_logger.warn("Failed to parse a link '#{link.link}' on '#{crawl_result.url}': #{link.error}")
./crawler/coordinator.rb:        system_logger.warn(log)
./crawler/coordinator.rb:        system_logger.fatal(log)
./crawler/coordinator.rb:          system_logger.warn(<<~LOG.squish)
./crawler/coordinator.rb:      system_logger.debug("Added #{added_urls_count} URLs from a #{source_type} source to the queue...")
./crawler/coordinator.rb:      system_logger.debug("Failed to add a crawler task into the processing queue: #{e}")
./es/client.rb:    def initialize(es_config, system_logger, crawler_version, crawl_id, &)
./es/client.rb:      @system_logger = system_logger
./es/client.rb:          @system_logger.info(<<~LOG.squish)
./es/client.rb:          @system_logger.warn(<<~LOG.squish)
./es/client.rb:            @system_logger.debug("Search attempt #{retries} failed. Retrying...")
./es/client.rb:            @system_logger.warn("Search failed after #{retries} attempts. #{e.message}")
./es/client.rb:            @system_logger.debug("Delete by query attempt #{retries} failed. Retrying...")
./es/client.rb:            @system_logger.warn("Delete by query failed after #{retries} attempts. #{e.message}")
./es/client.rb:        @system_logger.info('ES connections will be authorized with configured API key')
./es/client.rb:        @system_logger.info('ES connections will be authorized with configured username and password')
./es/client.rb:        @system_logger.info('ES connections will use SSL with ca_fingerprint')
./es/client.rb:      @system_logger.info('ES connections will use SSL without ca_fingerprint')
./es/client.rb:        @system_logger.warn("Errors found in bulk response. Full response: #{response}")
./es/client.rb:        @system_logger.debug('No errors found in bulk response.')
./es/client.rb:      @system_logger.warn("Saved failed bulk payload to #{full_path}")
./es/bulk_queue.rb:    def initialize(op_count_threshold, size_threshold, system_logger)
./es/bulk_queue.rb:      @system_logger = system_logger
./es/bulk_queue.rb:      @system_logger.debug(
./es/bulk_queue.rb:        @system_logger.error(log)
